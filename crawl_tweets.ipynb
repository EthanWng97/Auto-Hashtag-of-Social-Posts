{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "crawl tweets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1NWjg_y5u9bwPC2KEAuTY35csIoumVNhX",
      "authorship_tag": "ABX9TyPinU6zmIj0pjDTNzHtzeWI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NavePnow/Auto-Hashtag-of-Social-Posts/blob/master/crawl_tweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrAWuPIzPAd7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import tweepy as tw\n",
        "import pandas as pd\n",
        "import csv\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q49Ek4lfPCh5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "consumer_key = ''\n",
        "consumer_secret = ''\n",
        "access_token = ''\n",
        "access_token_secret = ''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83enZ5NSPRjj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def judge_pure_english(keyword):\n",
        "    return all(ord(c) < 128 for c in keyword)\n",
        "\n",
        "\n",
        "class TweetCrawl:\n",
        "    \"\"\" class TweetCrawl is a class dealing with crawling tweets\n",
        "    Args:\n",
        "        in_dir: working directory\n",
        "        consumer_key: Twitter API: consumer_key\n",
        "        consumer_secret: Twitter API: consumer_secret\n",
        "        access_token: Twitter API: access_token\n",
        "        access_token_secret: Twitter API: access_token_secret\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_dir, consumer_key, consumer_secret, access_token, access_token_secret):\n",
        "        self.in_dir = in_dir\n",
        "        self.consumer_key = consumer_key\n",
        "        self.consumer_secret = consumer_secret\n",
        "        self.access_token = access_token\n",
        "        self.access_token_secret = access_token_secret\n",
        "        self.auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
        "        self.auth.set_access_token(access_token, access_token_secret)\n",
        "        self.api = tw.API(self.auth, wait_on_rate_limit=True)\n",
        "\n",
        "    \"\"\" get tweets from Twitter API\n",
        "    Args:\n",
        "        extend: shorten the tweet or not\n",
        "        time: beginning time of cralwing\n",
        "        item: number of tweets will be crawled\n",
        "        expression: query search\n",
        "    Returns:\n",
        "        tweets: ItemIterator of tweets\n",
        "    \"\"\"\n",
        "\n",
        "    def _get_tweet(self, extend, time, item, expression):\n",
        "        if (extend == False):\n",
        "            tweets = tw.Cursor(self.api.search, q=expression,\n",
        "                               lang=\"en\",\n",
        "                               since=time,\n",
        "                               include_entities=True\n",
        "                               ).items(item)\n",
        "        else:\n",
        "            tweets = tw.Cursor(self.api.search, q=expression,\n",
        "                               lang=\"en\",\n",
        "                               since=time,\n",
        "                               tweet_mode=\"extended\",\n",
        "                               include_entities=True\n",
        "                               ).items()\n",
        "        return tweets\n",
        "\n",
        "    \"\"\" crawl train text\n",
        "    Args:\n",
        "        extend: shorten the tweet or not\n",
        "        time: beginning time of cralwing\n",
        "        item: number of tweets will be crawled\n",
        "        expression: query search\n",
        "    \"\"\"\n",
        "\n",
        "    def crawl_train_text(self, extend, time, item, expression):\n",
        "        file_dir = self.in_dir + 'input.train.text.csv'\n",
        "        csvFile = open(file_dir, 'a+')\n",
        "        csvWriter = csv.writer(csvFile)\n",
        "        print(\"crawl training text data\", end=\"\")\n",
        "        count = 0\n",
        "        tweets = self._get_tweet(extend, time, item, expression)\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                tweet = tweets.next()\n",
        "                count = count + 1\n",
        "                if (count % 50 == 0):\n",
        "                    print(\".\", end='', flush=True)\n",
        "                str = \"\"\n",
        "                if(tweet.entities['hashtags']):\n",
        "                    for item in tweet.entities['hashtags']:\n",
        "                        if not judge_pure_english(item['text']):\n",
        "                            continue\n",
        "                        str = str + ', ' + item['text']\n",
        "                    str = str.replace(', ', '', 1)\n",
        "                if((extend == False) and (\"RT\" not in tweet.text) and (str)):\n",
        "                    csvWriter.writerow([tweet.text, str])\n",
        "                if((extend == True) and (\"RT\" not in tweet.full_text) and (str)):\n",
        "                    csvWriter.writerow([tweet.full_text, str])\n",
        "            except tw.RateLimitError:\n",
        "                time.sleep(60 * 15)\n",
        "                continue\n",
        "            except StopIteration:\n",
        "                break\n",
        "        print(\"\\n\")\n",
        "\n",
        "    \"\"\" crawl test text\n",
        "    Args:\n",
        "        extend: shorten the tweet or not\n",
        "        time: beginning time of cralwing\n",
        "        item: number of tweets will be crawled\n",
        "    \"\"\"\n",
        "\n",
        "    def crawl_test_text(self, extend, time, item):\n",
        "        file_dir = self.in_dir + 'input.test.text.csv'\n",
        "        csvFile = open(file_dir, 'a+')\n",
        "        csvWriter = csv.writer(csvFile)\n",
        "        print(\"crawl testing text data\", end=\"\")\n",
        "        count = 0\n",
        "        tweets = self._get_tweet(extend, time, item)\n",
        "        while True:\n",
        "            try:\n",
        "                tweet = tweets.next()\n",
        "                count = count + 1\n",
        "                print(count)\n",
        "                if (count % 50 == 0):\n",
        "                    print(\".\", end='', flush=True)\n",
        "                str = \"\"\n",
        "                if(tweet.entities['hashtags']):\n",
        "                    for item in tweet.entities['hashtags']:\n",
        "                        str = str + ', ' + item['text']\n",
        "                    str = str.replace(', ', '', 1)\n",
        "                if((extend == False) and (\"RT\" not in tweet.text) and not(str)):\n",
        "                    csvWriter.writerow([tweet.text, str])\n",
        "                if((extend == True) and (\"RT\" not in tweet.full_text) and not(str)):\n",
        "                    csvWriter.writerow([tweet.full_text, str])\n",
        "            except tw.RateLimitError:\n",
        "                time.sleep(60 * 15)\n",
        "                continue\n",
        "            except StopIteration:\n",
        "                break\n",
        "        print(\"\\n\")\n",
        "\n",
        "    \"\"\" crawl train photo\n",
        "    Args:\n",
        "        time: beginning time of cralwing\n",
        "        item: number of tweets will be crawled\n",
        "    \"\"\"\n",
        "\n",
        "    def crawl_train_photo(self, time, item):\n",
        "        file_dir = self.in_dir + 'input.train.photo.csv'\n",
        "        csvFile = open(file_dir, 'a+')        \n",
        "        csvWriter = csv.writer(csvFile)\n",
        "        print(\"crawl training photo data\", end=\"\")\n",
        "        count = 0\n",
        "        tweets = self._get_tweet(False, time, item)\n",
        "        while True:\n",
        "            try:\n",
        "                tweet = tweets.next()\n",
        "                count = count + 1\n",
        "                if (count % 50 == 0):\n",
        "                    print(\".\", end='', flush=True)\n",
        "\n",
        "                str_photo = \"\"\n",
        "                if((\"RT\" not in tweet.text)):\n",
        "                    if('media' in tweet.entities.keys()):\n",
        "                        for item in tweet.entities['media']:\n",
        "                            str_photo = str_photo + ', ' + \\\n",
        "                                item['media_url_https']\n",
        "                str_photo = str_photo.replace(', ', '', 1)\n",
        "                if (not str_photo):\n",
        "                    continue  # filter out non-media tweet\n",
        "\n",
        "                str_tag = \"\"\n",
        "                if(tweet.entities['hashtags']):\n",
        "                    for item in tweet.entities['hashtags']:\n",
        "                        if not judge_pure_english(item['text']):\n",
        "                            continue\n",
        "                        str_tag = str_tag + ', ' + item['text']\n",
        "                    str_tag = str_tag.replace(', ', '', 1)\n",
        "                if(str_tag):\n",
        "                    csvWriter.writerow([str_photo, str_tag])\n",
        "            except tw.RateLimitError:\n",
        "                time.sleep(60 * 15)\n",
        "                continue\n",
        "            except StopIteration:\n",
        "                break\n",
        "        print(\"\\n\")\n",
        "\n",
        "    \"\"\" crawl test photo\n",
        "    Args:\n",
        "        time: beginning time of cralwing\n",
        "        item: number of tweets will be crawled\n",
        "    \"\"\"\n",
        "\n",
        "    def crawl_test_photo(self, time, item):\n",
        "        file_dir = self.in_dir + 'input.test.photo.csv'\n",
        "        csvFile = open(file_dir, 'a+')        \n",
        "        csvWriter = csv.writer(csvFile)\n",
        "        print(\"crawl testing photo data\", end=\"\")\n",
        "        count = 0\n",
        "        tweets = self._get_tweet(False, time, item)\n",
        "        while True:\n",
        "            try:\n",
        "                tweet = tweets.next()\n",
        "                count = count + 1\n",
        "                if (count % 50 == 0):\n",
        "                    print(\".\", end='', flush=True)\n",
        "\n",
        "                str_photo = \"\"\n",
        "                if((\"RT\" not in tweet.text)):\n",
        "                    if('media' in tweet.entities.keys()):\n",
        "                        for item in tweet.entities['media']:\n",
        "                            str_photo = str_photo + ', ' + \\\n",
        "                                item['media_url_https']\n",
        "                str_photo = str_photo.replace(', ', '', 1)\n",
        "                if (not str_photo):\n",
        "                    continue  # filter out non-media tweet\n",
        "\n",
        "                str_tag = \"\"\n",
        "                if(tweet.entities['hashtags']):\n",
        "                    for item in tweet.entities['hashtags']:\n",
        "                        str_tag = str_tag + ', ' + item['text']\n",
        "                    str_tag = str_tag.replace(', ', '', 1)\n",
        "                if(not(str_tag)):\n",
        "                    csvWriter.writerow([str_photo, str_tag])\n",
        "            except tw.RateLimitError:\n",
        "                time.sleep(60 * 15)\n",
        "                continue\n",
        "            except StopIteration:\n",
        "                break\n",
        "        print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HepuUcQK2TF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "in_dir = ''\n",
        "expression = \"#friends OR #pets OR #tuesdaymotivation OR #funny OR #contest OR #giveaway OR #ootd OR #win OR #merrychristmas OR #competition OR #fridayfeeling OR #traveltuesday OR #happybirthday OR #wcw OR #goals OR #fitness OR #vegan OR #movies OR #running OR #thankful OR #science OR #blessed OR #influencer OR #metoo\"\n",
        "tweet_crawl = TweetCrawl(\n",
        "    in_dir, consumer_key, consumer_secret, access_token, access_token_secret)\n",
        "tweet_crawl.crawl_train_text(\n",
        "    extend=True, time=\"2017-08-13\", item=500000, expression=expression)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}